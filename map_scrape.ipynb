{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of maps.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRkGxoW8zPsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1\n",
        "#Don't have to run this for different queries just run it once every time when you open the link\n",
        "\n",
        "#DON'T CHANGE THIS\n",
        "\n",
        "# setting the enviroment\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential chrpath libssl-dev libxft-dev\n",
        "!sudo apt-get install libfreetype6 libfreetype6-dev libfontconfig1 libfontconfig1-dev\n",
        "!wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2\n",
        "!tar xvjf phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/\n",
        "!sudo ln -sf /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin\n",
        "!phantomjs --version\n",
        "!pip install requests\n",
        "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "# !apt install chromium-chromedriver\n",
        "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "# !pip install selenium\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMsS2NfoU0Xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2\n",
        "#run this everytime you change the variables and then run the #3\n",
        "#setting the variables\n",
        "# set all the variables \n",
        "query=\"cannabis+dispensaries+oakland\" #set the query to search for NO SPACES allow use + instead of spaces\n",
        "pagesupto = 50\n",
        "keepScraping= True #change it to True if you don't want to answer yes/no each time \n",
        "domainList={'com','net','org'} #accept the emails ending with these\n",
        "addressOfContactPage = {'/pages/contact','/contact-us/'} # for an example website www.google.com the code will check 1. www.google.com/pages/contact 2. www.google.com/contact-us/ 3. www.google.com    to find email\n",
        "csvFileName='sample.csv' #name of file to save \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCYq1hxio0lq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3\n",
        "#Don't change this\n",
        "#run it after setting the variables \n",
        "from selenium import webdriver\n",
        "import re\n",
        "import requests\n",
        "import pandas\n",
        "from tqdm import tqdm_notebook as tqdmn\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "pagesupto-=1\n",
        "\n",
        "browser = webdriver.PhantomJS(desired_capabilities={'browserName': 'phantomjs', 'version': '', 'platform': 'ANY', 'javascriptEnabled': False}) # or add to your PATH\n",
        "\n",
        "finalans=[]\n",
        "browser.get('https://www.google.com/search?q=hi')\n",
        "\n",
        "# Defining all the functions\n",
        "\n",
        "def parse(place):\n",
        "    toret = {}\n",
        "    toret['link'] = place.get_attribute('href')\n",
        "    toret['name'] = place.find_element_by_class_name('BNeawe').text\n",
        "    toret['ratings'] = place.find_element_by_class_name('oqSTJd').text\n",
        "    spanElm = place.find_elements_by_css_selector('span')\n",
        "    for span in spanElm:\n",
        "        if span.get_attribute('class') =='':\n",
        "            toret['rated by'] = span.text.strip('(').strip(')').replace(',','')\n",
        "    toret['rated by'] = toret['rated by']\n",
        "    toret['extra data'] = place.text.split('\\n')[3:]\n",
        "    return toret\n",
        "\n",
        "def filterEmail(lst):\n",
        "    filst=[]\n",
        "    for email in lst:\n",
        "        if email.split('.')[-1] in domainList:\n",
        "            filst.append(email)\n",
        "    return filst\n",
        "def findEmail(source):\n",
        "    lst = set(re.findall('[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', source))\n",
        "    \n",
        "    return filterEmail(lst)\n",
        "def parseplaces(places):\n",
        "    toret = []\n",
        "    for place in places:\n",
        "        toret.append(parse(place))\n",
        "    return toret\n",
        "def parseCurrPage():\n",
        "    places = browser.find_elements_by_class_name('tHmfQe')\n",
        "    return parseplaces(places)\n",
        "def parseQuery(query):\n",
        "    global finalans\n",
        "    toret = []\n",
        "    scraped=0\n",
        "    print('searching query')\n",
        "    for i in range(2):\n",
        "        browser.get(f'https://www.google.com/search?safe=active&ei=MS00XcDKG9HTz7sP6vqEmAc&q={query}&rlst=f&sa=X&ved=2ahUKEwiAp9-01sXjAhXR6XMBHWo9AXMQjGowAHoECAsQDA')\n",
        "    while True:\n",
        "        \n",
        "        info = parseCurrPage()\n",
        "        toret+=info\n",
        "        finalans+=info\n",
        "        \n",
        "        nextpage =  [elm for elm in browser.find_elements_by_class_name('qFvlD') if elm.text=='>']\n",
        "        \n",
        "        if browser.find_element_by_class_name('nBDE1b').text == 'Next >':\n",
        "            nextpage += browser.find_elements_by_class_name('nBDE1b')\n",
        "        \n",
        "        if len(nextpage) ==1:\n",
        "            \n",
        "            \n",
        "            print(scraped+1)\n",
        "            if scraped>=pagesupto:\n",
        "                print('pages search completed')\n",
        "                break\n",
        "            print('going to next page')\n",
        "            nextpage[0].click()\n",
        "            \n",
        "        elif len(nextpage)==0:\n",
        "            print('no more pages')\n",
        "            break\n",
        "        else:\n",
        "            print('Contact me. There is something wrong')\n",
        "        scraped+=1\n",
        "    print('getting emails')\n",
        "    for ans in tqdmn(range(len(toret))):\n",
        "#         print(ans)\n",
        "#         print(toret[ans]['link'])\n",
        "        temps= processPlaceSearchemail(toret[ans]['link'])\n",
        "        toret[ans]['email'],toret[ans]['website'] = temps['email'],temps['website']\n",
        "#         print('next one')\n",
        "    print('done')\n",
        "    return toret\n",
        "def processPlaceSearchemail(link):\n",
        "    toret = {}\n",
        "#     print('getting page')\n",
        "\n",
        "    browser.get(link)\n",
        "#     print('got it')\n",
        "    emails = []\n",
        "    i=1\n",
        "#     print('selecting element')\n",
        "    for something in range(10):\n",
        "        try:\n",
        "            divElm = browser.find_elements_by_css_selector('div.ZINbbc.xpd.uUPGi')[i]\n",
        "            linkElm = divElm.find_element_by_css_selector('div.skVgpb')\n",
        "            \n",
        "            break\n",
        "        except:\n",
        "            i+=1\n",
        "    try:\n",
        "        weblink = linkElm.find_elements_by_css_selector('a.VGHMXd')[-1]\n",
        "    except:\n",
        "        toret['email'] = None\n",
        "        toret['website'] = None\n",
        "        return toret\n",
        "    rawlink = weblink.get_attribute('href')\n",
        "    processedlink = rawlink[rawlink.find('http',4):rawlink.find('&sa=U')]\n",
        "    toret['website'] = processedlink\n",
        "    \n",
        "#     print('going to website')\n",
        "    try:\n",
        "        emails= findEmail(requests.get(processedlink+'/pages/contact').text)\n",
        "    except:\n",
        "        emails = []\n",
        "#     print('checking further options')\n",
        "    for address in addressOfContactPage:\n",
        "        try:\n",
        "            emails+=findEmail(requests.get(processedlink+address).text)\n",
        "        except:\n",
        "            pass\n",
        "#     print('found emails')\n",
        "    emails = set(emails)\n",
        "    \n",
        "    if(len(emails)!=0):\n",
        "        toret['email'] = emails\n",
        "    else:\n",
        "        toret['email'] = None\n",
        "    return toret\n",
        "#making the search\n",
        "\n",
        "finalquery = parseQuery(query)\n",
        "#processing data to save to file\n",
        "print('query completed, now processing to make file')\n",
        "\n",
        "j = [list(f.values()) for f in finalquery]\n",
        "j = [f[1:]+f[:1] for f in j]\n",
        "print('creating pandas dataframe')\n",
        "csv = pandas.DataFrame(j,columns=['name','ratings','rated by','extra info','email','website','link'])\n",
        "print('file ready to save')\n",
        "#save the file \n",
        "csv.to_csv(csvFileName)\n",
        "print('you can download the \"sample.csv\" file by going to the little arrow in top left corner just down to \"+Code\" button and then to files options.')\n",
        "print(\"contents of file:-\")\n",
        "csv"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
